{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445eb11c-b5c6-4d03-b6e1-bd6f250e4113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Extra Trees model\n",
    "model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75143c80-f655-4efe-b2bc-b58d275a6601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "log_clf = LogisticRegression(max_iter=1000)\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "# Create Voting Classifier (hard voting)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', tree_clf), ('knn', knn_clf)],\n",
    "    voting='hard'  # 'soft' for probability-based voting\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9179140d-6c06-4a71-950a-e107e3229f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2775.165076183445\n",
      "Coefficients: [   0.         -173.27107577  558.93812468  339.35373951  -58.72068535\n",
      "   -0.         -274.11351588    0.          372.83897776   25.58680152]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)  # alpha is the regularization strength (λ)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = lasso.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Coefficients:\", lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1901a1c-0bdf-46f7-82cb-001b95e729f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3112.9664151980814\n",
      "Coefficients: [  45.05421022  -71.94739737  280.71625182  195.21266175   -2.22930269\n",
      "  -17.54079744 -148.68886188  120.46723979  198.61440137  106.93469215]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load example data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)  # alpha = λ (regularization strength)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = ridge.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Coefficients:\", ridge.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855bf5eb-60f1-49b8-8239-2305b122cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2859.641982706767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)  # 100 trees\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f05dc5-e64b-4952-97ee-2374fbc466c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3071.448520446322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d36078e-326d-4791-b125-8fb501d32f3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_diabetes\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Use MSE as evaluation metric\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Create study and run optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b361cab-3ab4-455d-8ae2-fde0dfb4c8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (1.16.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7eb458-31a7-415e-ac21-aa0bb4850fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-12 20:51:57,926] A new study created in memory with name: no-name-85babe2f-efe1-4aa3-bd8c-82d96fb49fe1\n",
      "[I 2025-06-12 20:51:58,174] Trial 0 finished with value: 4464.977199319589 and parameters: {'n_estimators': 73, 'learning_rate': 0.24907493784504073, 'max_depth': 8, 'subsample': 0.6318170884734755}. Best is trial 0 with value: 4464.977199319589.\n",
      "[I 2025-06-12 20:51:58,907] Trial 1 finished with value: 2990.3949693365244 and parameters: {'n_estimators': 150, 'learning_rate': 0.029470445232774693, 'max_depth': 8, 'subsample': 0.5391284690127445}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:51:59,475] Trial 2 finished with value: 3370.497456032335 and parameters: {'n_estimators': 133, 'learning_rate': 0.19161258881349744, 'max_depth': 8, 'subsample': 0.612669385079472}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:51:59,698] Trial 3 finished with value: 3504.7688621655475 and parameters: {'n_estimators': 75, 'learning_rate': 0.20485365398102945, 'max_depth': 3, 'subsample': 0.8841954490310469}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:52:00,707] Trial 4 finished with value: 3431.236670011172 and parameters: {'n_estimators': 184, 'learning_rate': 0.27936843180490356, 'max_depth': 8, 'subsample': 0.9584664281438666}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:52:01,357] Trial 5 finished with value: 3442.4550153088526 and parameters: {'n_estimators': 265, 'learning_rate': 0.07327085582909527, 'max_depth': 4, 'subsample': 0.7991930419749607}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:52:01,974] Trial 6 finished with value: 3508.375305379082 and parameters: {'n_estimators': 223, 'learning_rate': 0.24067360207963565, 'max_depth': 2, 'subsample': 0.9736968933848618}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:52:02,890] Trial 7 finished with value: 3766.6087320093575 and parameters: {'n_estimators': 153, 'learning_rate': 0.23140657904994302, 'max_depth': 10, 'subsample': 0.747096812486397}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:52:03,435] Trial 8 finished with value: 3397.906830313009 and parameters: {'n_estimators': 170, 'learning_rate': 0.06416903867635065, 'max_depth': 5, 'subsample': 0.7621790762203273}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:52:03,750] Trial 9 finished with value: 3491.4174553496737 and parameters: {'n_estimators': 140, 'learning_rate': 0.17078551934371952, 'max_depth': 5, 'subsample': 0.5332444361349666}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:52:04,689] Trial 10 finished with value: 3076.2752404309445 and parameters: {'n_estimators': 289, 'learning_rate': 0.022413077761433822, 'max_depth': 10, 'subsample': 0.5073639732049052}. Best is trial 1 with value: 2990.3949693365244.\n",
      "[I 2025-06-12 20:52:05,609] Trial 11 finished with value: 2899.2089214639523 and parameters: {'n_estimators': 282, 'learning_rate': 0.010694768282965, 'max_depth': 10, 'subsample': 0.5238117099774579}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:06,387] Trial 12 finished with value: 2943.8820504540154 and parameters: {'n_estimators': 223, 'learning_rate': 0.010203449130234334, 'max_depth': 7, 'subsample': 0.6220438318496393}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:07,266] Trial 13 finished with value: 3228.273504713569 and parameters: {'n_estimators': 231, 'learning_rate': 0.11439478698372459, 'max_depth': 6, 'subsample': 0.6362240262825806}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:08,358] Trial 14 finished with value: 3590.7232152987262 and parameters: {'n_estimators': 250, 'learning_rate': 0.11288090277155503, 'max_depth': 9, 'subsample': 0.6881916983246764}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:09,381] Trial 15 finished with value: 2973.3035850478345 and parameters: {'n_estimators': 297, 'learning_rate': 0.01238781406745733, 'max_depth': 7, 'subsample': 0.5850873136169343}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:10,044] Trial 16 finished with value: 3274.856434069696 and parameters: {'n_estimators': 205, 'learning_rate': 0.06358962304982589, 'max_depth': 6, 'subsample': 0.6907574500890827}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:11,259] Trial 17 finished with value: 3583.6850276720315 and parameters: {'n_estimators': 267, 'learning_rate': 0.13871657112830815, 'max_depth': 10, 'subsample': 0.5748620983949253}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:12,143] Trial 18 finished with value: 3441.641820485196 and parameters: {'n_estimators': 243, 'learning_rate': 0.09153844207994634, 'max_depth': 7, 'subsample': 0.7044027238710638}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:12,997] Trial 19 finished with value: 3431.134464154054 and parameters: {'n_estimators': 205, 'learning_rate': 0.045682555691049426, 'max_depth': 9, 'subsample': 0.8299083606191614}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:13,390] Trial 20 finished with value: 3131.4729017675354 and parameters: {'n_estimators': 117, 'learning_rate': 0.0985668154781961, 'max_depth': 9, 'subsample': 0.5002706050597827}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:14,240] Trial 21 finished with value: 3131.1590009077277 and parameters: {'n_estimators': 298, 'learning_rate': 0.02159197735959354, 'max_depth': 7, 'subsample': 0.5784981734758543}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:15,392] Trial 22 finished with value: 3125.2658514612604 and parameters: {'n_estimators': 280, 'learning_rate': 0.051921876681495324, 'max_depth': 7, 'subsample': 0.5997584568463697}. Best is trial 11 with value: 2899.2089214639523.\n",
      "[I 2025-06-12 20:52:16,187] Trial 23 finished with value: 2887.667246991767 and parameters: {'n_estimators': 300, 'learning_rate': 0.010075907881795888, 'max_depth': 5, 'subsample': 0.6584415702164362}. Best is trial 23 with value: 2887.667246991767.\n",
      "[I 2025-06-12 20:52:17,135] Trial 24 finished with value: 3302.5945881646508 and parameters: {'n_estimators': 264, 'learning_rate': 0.03732089150837417, 'max_depth': 5, 'subsample': 0.6678300313060799}. Best is trial 23 with value: 2887.667246991767.\n",
      "[I 2025-06-12 20:52:17,668] Trial 25 finished with value: 2880.4717221347073 and parameters: {'n_estimators': 221, 'learning_rate': 0.014097389128093758, 'max_depth': 4, 'subsample': 0.6499192601428351}. Best is trial 25 with value: 2880.4717221347073.\n",
      "[I 2025-06-12 20:52:18,374] Trial 26 finished with value: 3222.9867149824686 and parameters: {'n_estimators': 277, 'learning_rate': 0.07851274037365563, 'max_depth': 4, 'subsample': 0.7420424734566485}. Best is trial 25 with value: 2880.4717221347073.\n",
      "[I 2025-06-12 20:52:18,929] Trial 27 finished with value: 3173.313752982578 and parameters: {'n_estimators': 246, 'learning_rate': 0.046223280508079743, 'max_depth': 4, 'subsample': 0.6603478880180821}. Best is trial 25 with value: 2880.4717221347073.\n",
      "[I 2025-06-12 20:52:19,335] Trial 28 finished with value: 3288.9888388601594 and parameters: {'n_estimators': 193, 'learning_rate': 0.14766230908269243, 'max_depth': 3, 'subsample': 0.719439883738378}. Best is trial 25 with value: 2880.4717221347073.\n",
      "[I 2025-06-12 20:52:19,490] Trial 29 finished with value: 2730.703653296573 and parameters: {'n_estimators': 61, 'learning_rate': 0.04250765753961053, 'max_depth': 3, 'subsample': 0.5357997447591657}. Best is trial 29 with value: 2730.703653296573.\n",
      "[I 2025-06-12 20:52:19,639] Trial 30 finished with value: 3557.494449680475 and parameters: {'n_estimators': 65, 'learning_rate': 0.2989669018946803, 'max_depth': 2, 'subsample': 0.6482630303810724}. Best is trial 29 with value: 2730.703653296573.\n",
      "[I 2025-06-12 20:52:19,861] Trial 31 finished with value: 2760.4312261842065 and parameters: {'n_estimators': 100, 'learning_rate': 0.034598070563563135, 'max_depth': 3, 'subsample': 0.5507912807229349}. Best is trial 29 with value: 2730.703653296573.\n",
      "[I 2025-06-12 20:52:20,063] Trial 32 finished with value: 2726.106629416108 and parameters: {'n_estimators': 86, 'learning_rate': 0.03485504865059848, 'max_depth': 3, 'subsample': 0.5530210747153936}. Best is trial 32 with value: 2726.106629416108.\n",
      "[I 2025-06-12 20:52:20,268] Trial 33 finished with value: 2726.617509333641 and parameters: {'n_estimators': 91, 'learning_rate': 0.03839818064932154, 'max_depth': 3, 'subsample': 0.5552516512606288}. Best is trial 32 with value: 2726.106629416108.\n",
      "[I 2025-06-12 20:52:20,474] Trial 34 finished with value: 2670.449568629382 and parameters: {'n_estimators': 89, 'learning_rate': 0.03521384701677072, 'max_depth': 3, 'subsample': 0.5464873343334717}. Best is trial 34 with value: 2670.449568629382.\n",
      "[I 2025-06-12 20:52:20,615] Trial 35 finished with value: 2706.6571330319753 and parameters: {'n_estimators': 52, 'learning_rate': 0.056334917866586995, 'max_depth': 3, 'subsample': 0.5584243862623889}. Best is trial 34 with value: 2670.449568629382.\n",
      "[I 2025-06-12 20:52:20,800] Trial 36 finished with value: 2744.0508141824503 and parameters: {'n_estimators': 88, 'learning_rate': 0.08371742492552497, 'max_depth': 2, 'subsample': 0.5586128685137861}. Best is trial 34 with value: 2670.449568629382.\n",
      "[I 2025-06-12 20:52:20,943] Trial 37 finished with value: 2638.03163100542 and parameters: {'n_estimators': 53, 'learning_rate': 0.06337140621313908, 'max_depth': 2, 'subsample': 0.6090645105793709}. Best is trial 37 with value: 2638.03163100542.\n",
      "[I 2025-06-12 20:52:21,085] Trial 38 finished with value: 2706.544654605028 and parameters: {'n_estimators': 56, 'learning_rate': 0.0604516234357728, 'max_depth': 2, 'subsample': 0.6051953671442254}. Best is trial 37 with value: 2638.03163100542.\n",
      "[I 2025-06-12 20:52:21,280] Trial 39 finished with value: 2642.3768008778816 and parameters: {'n_estimators': 51, 'learning_rate': 0.11530956857827154, 'max_depth': 2, 'subsample': 0.6017011428094939}. Best is trial 37 with value: 2638.03163100542.\n",
      "[I 2025-06-12 20:52:21,508] Trial 40 finished with value: 2882.5612346951934 and parameters: {'n_estimators': 111, 'learning_rate': 0.12941344357733348, 'max_depth': 2, 'subsample': 0.6178953817729489}. Best is trial 37 with value: 2638.03163100542.\n",
      "[I 2025-06-12 20:52:21,639] Trial 41 finished with value: 2638.539874264787 and parameters: {'n_estimators': 50, 'learning_rate': 0.0645974877637765, 'max_depth': 2, 'subsample': 0.5981338855979009}. Best is trial 37 with value: 2638.03163100542.\n",
      "[I 2025-06-12 20:52:21,818] Trial 42 finished with value: 2818.9871614600534 and parameters: {'n_estimators': 73, 'learning_rate': 0.10519214656112373, 'max_depth': 2, 'subsample': 0.917672107315651}. Best is trial 37 with value: 2638.03163100542.\n",
      "[I 2025-06-12 20:52:21,955] Trial 43 finished with value: 2741.864322770204 and parameters: {'n_estimators': 53, 'learning_rate': 0.16528110134258178, 'max_depth': 2, 'subsample': 0.6011762047023794}. Best is trial 37 with value: 2638.03163100542.\n",
      "[I 2025-06-12 20:52:22,121] Trial 44 finished with value: 2536.6751054540596 and parameters: {'n_estimators': 75, 'learning_rate': 0.06934826204840111, 'max_depth': 2, 'subsample': 0.5997223989098714}. Best is trial 44 with value: 2536.6751054540596.\n",
      "[I 2025-06-12 20:52:22,288] Trial 45 finished with value: 2674.7196316897475 and parameters: {'n_estimators': 73, 'learning_rate': 0.0721124423011162, 'max_depth': 2, 'subsample': 0.5202666800135859}. Best is trial 44 with value: 2536.6751054540596.\n",
      "[I 2025-06-12 20:52:22,478] Trial 46 finished with value: 3003.654468841219 and parameters: {'n_estimators': 78, 'learning_rate': 0.12311048063821933, 'max_depth': 3, 'subsample': 0.5835281446727393}. Best is trial 44 with value: 2536.6751054540596.\n",
      "[I 2025-06-12 20:52:22,689] Trial 47 finished with value: 3060.938483995699 and parameters: {'n_estimators': 104, 'learning_rate': 0.2053972200358008, 'max_depth': 2, 'subsample': 0.6250489962230225}. Best is trial 44 with value: 2536.6751054540596.\n",
      "[I 2025-06-12 20:52:23,031] Trial 48 finished with value: 3128.3344391206615 and parameters: {'n_estimators': 125, 'learning_rate': 0.09190700062432085, 'max_depth': 4, 'subsample': 0.8447066669338968}. Best is trial 44 with value: 2536.6751054540596.\n",
      "[I 2025-06-12 20:52:23,195] Trial 49 finished with value: 2737.400235246302 and parameters: {'n_estimators': 66, 'learning_rate': 0.07051146439501396, 'max_depth': 2, 'subsample': 0.7723961520489452}. Best is trial 44 with value: 2536.6751054540596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'n_estimators': 75, 'learning_rate': 0.06934826204840111, 'max_depth': 2, 'subsample': 0.5997223989098714}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Use MSE as evaluation metric\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Create study and run optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6015f6-0c1b-4e7c-a442-98a172e7a515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
